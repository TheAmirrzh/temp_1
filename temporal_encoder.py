"""
Temporal State Encoder for Neural Theorem Proving
=================================================

State-of-the-art temporal encoding for proof state evolution, based on:
- TGN (Rossi et al., ICLR 2020): Temporal Graph Networks
- DyGFormer (Yu et al., 2023): Transformer for temporal graphs
- GraphMixer (Cong et al., 2023): Fixed time encoding
- T-PE (2024): Temporal Positional Encoding with geometric + semantic components
- tAPE (Foumani et al., 2023): Time-aware absolute positional encoding

Key innovations for theorem proving:
1. Derivation-aware temporal encoding (tracks which nodes derived at which steps)
2. Proof frontier attention (focus on recently derived facts)
3. Multi-scale temporal context (captures both local and global proof dynamics)
4. Fixed sinusoidal encoding (stable, no training instability)

Author: AI Research Team
Date: October 2025
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Optional, Tuple, Dict, List
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class FixedTimeEncoding(nn.Module):
    """
    Fixed sinusoidal time encoding adapted for proof steps.
    
    Based on GraphMixer (Cong et al., 2023) and tAPE (Foumani et al., 2023).
    Uses fixed frequencies for stability during training.
    
    Formula: PE(t, 2i) = cos(t * ω_i)
              PE(t, 2i+1) = sin(t * ω_i)
    where ω_i = 1 / (max_steps^(2i/d))
    """
    
    def __init__(self, d_model: int, max_steps: int = 1000):
        """
        Initialize fixed time encoding.
        
        Args:
            d_model: Embedding dimension (must be even)
            max_steps: Maximum number of proof steps expected
        """
        super().__init__()
        assert d_model % 2 == 0, "d_model must be even for sinusoidal encoding"
        
        self.d_model = d_model
        self.max_steps = max_steps
        
        # Precompute frequency terms (fixed, not learnable)
        # Adapted tAPE formula: incorporate max_steps for better scaling
        position = torch.arange(d_model // 2, dtype=torch.float32)
        freq_term = 1.0 / (max_steps ** (2 * position / d_model))
        self.register_buffer('freq_term', freq_term)
        
        logger.info(f"FixedTimeEncoding initialized: d_model={d_model}, max_steps={max_steps}")
    
    def forward(self, step_numbers: torch.Tensor) -> torch.Tensor:
        """
        Encode proof step numbers.
        
        Args:
            step_numbers: [N] tensor of step numbers (0 to max_steps)
        
        Returns:
            [N, d_model] tensor of time encodings
        """
        # Clamp steps to handle cases where step > max_steps
        step_clamped = torch.clamp(step_numbers.float(), 0, self.max_steps)
        
        # Compute angles: step * ω
        # Shape: [N, d_model//2]
        angles = step_clamped.unsqueeze(-1) * self.freq_term.unsqueeze(0)
        
        # Apply sin and cos
        sin_encoding = torch.sin(angles)
        cos_encoding = torch.cos(angles)
        
        # Interleave: [cos, sin, cos, sin, ...]
        encoding = torch.stack([cos_encoding, sin_encoding], dim=-1)
        encoding = encoding.reshape(step_numbers.shape[0], self.d_model)
        
        return encoding


class ProofFrontierAttention(nn.Module):
    """
    Attention mechanism focused on proof frontier (recently derived facts).
    
    Based on TGN (Rossi et al., 2020) mailbox mechanism and
    DyGFormer (Yu et al., 2023) historical interaction encoding.
    
    Key insight: Recent derivations are more relevant for next-step prediction.
    """
    
    def __init__(
        self,
        hidden_dim: int,
        num_heads: int = 4,
        frontier_window: int = 5,
        dropout: float = 0.1
    ):
        """
        Initialize frontier attention.
        
        Args:
            hidden_dim: Hidden dimension
            num_heads: Number of attention heads
            frontier_window: How many recent steps to focus on
            dropout: Dropout rate
        """
        super().__init__()
        
        self.frontier_window = frontier_window
        self.attention = nn.MultiheadAttention(
            hidden_dim,
            num_heads,
            dropout=dropout,
            batch_first=True
        )
        self.layer_norm = nn.LayerNorm(hidden_dim)
        
        logger.info(f"ProofFrontierAttention initialized: "
                    f"heads={num_heads}, window={frontier_window}")
    
    def forward(
        self,
        x: torch.Tensor,
        step_numbers: torch.Tensor,
        max_step: int
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Apply frontier-focused attention.
        
        Args:
            x: [N, hidden_dim] node features
            step_numbers: [N] step when each node was derived (0 = axiom)
            max_step: Current maximum step number
        
        Returns:
            attended_features: [N, hidden_dim]
            attention_weights: [N, N] attention scores
        """
        # Create frontier mask: nodes derived in last K steps
        frontier_threshold = max(0, max_step - self.frontier_window)
        # Frontier nodes are those derived *after* the threshold, AND are not axioms
        is_frontier = (step_numbers > frontier_threshold)
        
        # If no frontier nodes, attend to all derived nodes (non-axioms)
        if not is_frontier.any():
            is_frontier = step_numbers > 0
            # If still no derived nodes, attend to all (prevents empty mask)
            if not is_frontier.any():
                is_frontier = torch.ones_like(step_numbers, dtype=torch.bool)
        
        # Create attention mask: only attend to frontier nodes
        # Shape: [N, N] where True = mask out (don't attend)
        attn_mask = ~is_frontier.unsqueeze(0).expand(x.shape[0], -1)
        
        # Apply multi-head attention
        # query=key=value=x (self-attention)
        attended, attn_weights = self.attention(
            x.unsqueeze(0),  # Add batch dim
            x.unsqueeze(0),
            x.unsqueeze(0),
            attn_mask=attn_mask, # MHA automatically adds batch dim to mask
            need_weights=True
        )
        
        attended = attended.squeeze(0)  # Remove batch dim
        
        # Residual connection + layer norm
        output = self.layer_norm(x + attended)
        
        return output, attn_weights.squeeze(0)


class TemporalStateEncoder(nn.Module):
    """
    Complete temporal state encoder for proof evolution.
    
    Combines:
    1. Derivation status embedding (axiom vs derived)
    2. Fixed time encoding for step numbers (stable)
    3. Proof frontier attention (recent derivations)
    4. Multi-scale temporal aggregation (local + global context)
    
    Architecture based on T-PE (2024): Geometric + Semantic components.
    """
    
    def __init__(
        self,
        hidden_dim: int = 256,
        num_heads: int = 4,
        frontier_window: int = 5,
        max_steps: int = 100,
        dropout: float = 0.1
    ):
        """
        Initialize temporal state encoder.
        
        Args:
            hidden_dim: Hidden dimension for features
            num_heads: Number of attention heads for frontier attention
            frontier_window: Recent steps window for frontier
            max_steps: Maximum proof steps expected
            dropout: Dropout rate
        """
        super().__init__()
        
        self.hidden_dim = hidden_dim
        self.max_steps = max_steps
        
        # Component 1: Derivation status embedding
        # 0 = axiom (initial), 1 = derived
        self.status_embed = nn.Embedding(2, hidden_dim // 4)
        
        # Component 2: Fixed time encoding (geometric PE)
        self.time_encoder = FixedTimeEncoding(
            d_model=hidden_dim // 2,
            max_steps=max_steps
        )
        
        # Component 3: Frontier attention (semantic PE based on recency)
        self.frontier_attention = ProofFrontierAttention(
            hidden_dim=hidden_dim,
            num_heads=num_heads,
            frontier_window=frontier_window,
            dropout=dropout
        )
        
        # Component 4: Multi-scale aggregation
        # Local scale: recent steps (window-based)
        self.local_aggregator = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ELU(),
            nn.Dropout(dropout)
        )
        
        # Global scale: all proof history
        self.global_aggregator = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ELU(),
            nn.Dropout(dropout)
        )
        
        # Fusion layer: combine all components
        fusion_input_dim = (
            hidden_dim // 4 +  # Status embedding
            hidden_dim // 2 +  # Time encoding
            hidden_dim +     # Local context
            hidden_dim       # Global context
        )
        
        self.fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, hidden_dim * 2),
            nn.LayerNorm(hidden_dim * 2),
            nn.ELU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ELU()
        )
        
        logger.info(f"TemporalStateEncoder initialized: dim={hidden_dim}, "
                    f"heads={num_heads}, window={frontier_window}")
    
    def forward(
        self,
        derived_mask: torch.Tensor,
        step_numbers: torch.Tensor,
        node_features: torch.Tensor,
        return_attention: bool = False
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        Encode temporal state of proof.
        
        Args:
            derived_mask: [N] binary tensor (0=axiom, 1=derived)
            step_numbers: [N] step when node derived (0 for axioms)
            node_features: [N, hidden_dim] current node features (from structural GNN)
            return_attention: Whether to return attention weights
        
        Returns:
            temporal_encoding: [N, hidden_dim] temporal features
            attention_weights: [N, N] if return_attention=True, else None
        """
        num_nodes = derived_mask.shape[0]
        max_step = step_numbers.max().item()
        
        # Component 1: Status embedding
        status_emb = self.status_embed(derived_mask.long())  # [N, hidden_dim//4]
        
        # Component 2: Time encoding (geometric PE)
        time_emb = self.time_encoder(step_numbers)  # [N, hidden_dim//2]
        
        # Component 3: Frontier attention (semantic PE)
        frontier_features, attn_weights = self.frontier_attention(
            node_features,
            step_numbers,
            max_step
        )  # [N, hidden_dim]
        
        # Component 4: Multi-scale aggregation
        # Local context: weight recent derivations more
        # Create recency weights: exponential decay
        recency_weights = torch.exp(
            -(max_step - step_numbers.float()) / self.frontier_attention.frontier_window
        )
        recency_weights = recency_weights / (recency_weights.sum() + 1e-8)
        
        # Weighted aggregation for local context
        local_context = self.local_aggregator(frontier_features)
        local_weighted = local_context * recency_weights.unsqueeze(-1)
        
        # Global context: uniform aggregation over all derived nodes
        derived_indices = (derived_mask == 1).nonzero(as_tuple=True)[0]
        if len(derived_indices) > 0:
            global_pool = frontier_features[derived_indices].mean(dim=0, keepdim=True)
            global_pool = global_pool.expand(num_nodes, -1)
        else:
            # No derivations yet: use zeros
            global_pool = torch.zeros_like(frontier_features)
        
        global_context = self.global_aggregator(global_pool)
        
        # Fusion: Concatenate all temporal components
        # [N, (hidden//4) + (hidden//2) + hidden + hidden]
        fused_input = torch.cat([
            status_emb,      # Component 1: Axiom vs Derived
            time_emb,        # Component 2: Geometric time (absolute)
            local_weighted,  # Component 4a: Local context (recency)
            global_context   # Component 4b: Global context (history)
        ], dim=1)
        
        # Pass through final fusion network
        temporal_encoding = self.fusion(fused_input)  # [N, hidden_dim]
        
        if return_attention:
            return temporal_encoding, attn_weights
        else:
            return temporal_encoding, None


# --- ADDED MISSING CLASS ---
class MultiScaleTemporalEncoder(nn.Module):
    """
    Fuses temporal encodings from multiple time scales.
    Based on documentation: Fine, Medium, Coarse scales.
    """
    def __init__(self, 
                 hidden_dim: int, 
                 num_scales: int = 3, 
                 max_steps: int = 100, 
                 dropout: float = 0.1):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_scales = num_scales
        
        # Define the windows: e.g., [2, 5, max_steps]
        # This matches the "Fine, Medium, Coarse" in the README
        if num_scales > 1:
            self.temporal_windows = np.linspace(2, max_steps, num_scales, dtype=int)
            # Ensure the last window is always global
            self.temporal_windows[-1] = max_steps 
        else:
            self.temporal_windows = np.array([5]) # Default medium scale
        
        self.scale_encoders = nn.ModuleList()
        for window in self.temporal_windows:
            self.scale_encoders.append(
                TemporalStateEncoder(
                    hidden_dim=hidden_dim,
                    num_heads=4,
                    frontier_window=int(window), # Cast to int
                    max_steps=max_steps,
                    dropout=dropout
                )
            )
        
        # Fusion layer to combine S scales
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * num_scales, hidden_dim * 2),
            nn.ELU(),
            nn.LayerNorm(hidden_dim * 2),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ELU(),
            nn.LayerNorm(hidden_dim)
        )
        logger.info(f"MultiScaleTemporalEncoder initialized: {num_scales} scales, windows={self.temporal_windows}")

    def forward(self, 
                derived_mask: torch.Tensor, 
                step_numbers: torch.Tensor, 
                node_features: torch.Tensor) -> torch.Tensor:
        # Collect encodings from each scale
        scale_outputs = []
        for encoder in self.scale_encoders:
            enc, _ = encoder(derived_mask, step_numbers, node_features)
            scale_outputs.append(enc)
        
        # Concatenate along the feature dimension
        fused_input = torch.cat(scale_outputs, dim=1) # [N, hidden_dim * num_scales]
        
        # Fuse
        output = self.fusion(fused_input) # [N, hidden_dim]
        return output

# --- ADDED MISSING UTILITY FUNCTIONS ---

def compute_derived_mask(proof_state: Dict, current_step: int) -> torch.Tensor:
    """Computes binary mask of derived (non-axiom) nodes."""
    num_nodes = proof_state.get('num_nodes', 0)
    mask = torch.zeros(num_nodes, dtype=torch.uint8)
    derivations = proof_state.get('derivations', [])
    
    for node_idx, step in derivations:
        if node_idx < num_nodes and step <= current_step:
            mask[node_idx] = 1
    return mask

def compute_step_numbers(proof_state: Dict, current_step: int) -> torch.Tensor:
    """Computes step number for each node, 0 for axioms/future."""
    num_nodes = proof_state.get('num_nodes', 0)
    steps = torch.zeros(num_nodes, dtype=torch.long)
    derivations = proof_state.get('derivations', [])
    
    for node_idx, step in derivations:
        if node_idx < num_nodes and step <= current_step:
            steps[node_idx] = step
    return steps

def compute_derivation_dependencies(proof_state: Dict, current_step: int) -> torch.Tensor:
    """Computes adjacency matrix of derivation dependencies."""
    num_nodes = proof_state.get('num_nodes', 0)
    deps = torch.zeros(num_nodes, num_nodes, dtype=torch.float)
    dependencies = proof_state.get('dependencies', [])
    
    for derived_idx, parents, step in dependencies:
        if derived_idx < num_nodes and step <= current_step:
            for parent_idx in parents:
                if parent_idx < num_nodes:
                    deps[derived_idx, parent_idx] = 1.0
    return deps


# Example usage and testing
if __name__ == "__main__":
    print("Temporal State Encoder - Testing")
    print("=" * 60)
    
    N = 10  # 10 nodes
    hidden_dim = 128
    max_steps = 20
    
    # Initialize encoder
    encoder = TemporalStateEncoder(
        hidden_dim=hidden_dim,
        num_heads=4,
        frontier_window=5,
        max_steps=max_steps
    )
    
    # Create sample proof state
    # 3 axioms, 7 derived nodes
    derived_mask = torch.tensor([0, 0, 0, 1, 1, 1, 1, 1, 1, 1], dtype=torch.uint8)
    # Step numbers (axioms=0, others derived at various steps)
    # Current max step is 5
    step_numbers = torch.tensor([0, 0, 0, 1, 2, 2, 3, 4, 5, 5])
    # Initial node features
    node_features = torch.randn(N, hidden_dim)
    
    print(f"Test input: {N} nodes, current max_step={step_numbers.max().item()}")
    print(f"Derived mask: {derived_mask.tolist()}")
    print(f"Step numbers: {step_numbers.tolist()}")
    
    # Forward pass
    temporal_features, attn_weights = encoder(
        derived_mask,
        step_numbers,
        node_features,
        return_attention=True
    )
    
    print("\n--- Output ---")
    print(f"Temporal features shape: {temporal_features.shape}")
    print(f"Attention weights shape: {attn_weights.shape}")
    
    # Check shapes
    assert temporal_features.shape == (N, hidden_dim)
    assert attn_weights.shape == (N, N)
    
    # Check frontier attention
    # max_step=5, window=5 -> threshold=0
    # is_frontier = (step_numbers > 0)
    # Frontier nodes = indices [3, 4, 5, 6, 7, 8, 9]
    # Masked (key) nodes = indices [0, 1, 2]
    # (Note: attn_weights[i, j] = attention from node i to node j)
    # The mask applies to keys (j). Sum of weights to nodes 0, 1, 2 should be 0.
    non_frontier_attn = attn_weights[:, :3].sum()
    print(f"Sum of attention to non-frontier (should be 0): {non_frontier_attn.item()}")
    assert torch.allclose(non_frontier_attn, torch.tensor(0.0), atol=1e-6)
    
    print("\n" + "=" * 60)
    print("All tests passed! TemporalStateEncoder is ready.")