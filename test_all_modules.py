#!/usr/bin/env python3
"""
Comprehensive Test Suite for the Horn Clause GNN Project (Phase 2.5)

This script provides unit tests for each major module, verifying
core functionality, class initializations, method outputs, and tensor shapes.

Usage:
  python test_all_modules.py
"""

import unittest
import torch
import numpy as np
import json
import os
import shutil
from pathlib import Path
from torch_geometric.data import Data, Batch
from torch.utils.data import SubsetRandomSampler, DataLoader

# --- Import Project Modules ---
# Note: Ensure these files are in the same directory or Python path
try:
    from data_generator import generate_horn_instance, generate_dataset, Difficulty
    from dataset import StepPredictionDataset, create_split, RuleToTacticMapper
    from losses import ProofSearchRankingLoss, compute_hit_at_k, compute_mrr
    from model import (
        ImprovedSpectralFilter, TypeAwareGATv2Conv, NodeRankingGNN,
        FixedTemporalSpectralGNN, TacticClassifier, TacticGuidedGNN,
        PremiseSelector, get_model
    )
    from spectral_features import SpectralFeatureExtractor, load_spectral_features
    from temporal_encoder import (
        FixedTimeEncoding, ProofFrontierAttention, TemporalStateEncoder,
        MultiScaleTemporalEncoder, compute_derived_mask, compute_step_numbers
    )
    from search_agent import ProofSimulator, BeamSearchRanker
    from train import CurriculumScheduler, train_epoch, evaluate, get_lr, main as train_main # Import main for args testing
except ImportError as e:
    print(f"Error importing project modules: {e}")
    print("Please ensure all project .py files are in the Python path.")
    exit(1)

# --- Constants and Dummy Data Setup ---
TEST_DATA_DIR = Path("./test_temp_data")
TEST_SPECTRAL_DIR = TEST_DATA_DIR / "spectral_cache"
DUMMY_INSTANCE_FILE = TEST_DATA_DIR / "dummy_instance_medium_0.json"
DUMMY_SPECTRAL_FILE = TEST_SPECTRAL_DIR / "dummy_instance_medium_0_spectral.npz"

# Minimal valid dummy instance structure
DUMMY_INSTANCE = {
    "id": "dummy_instance_medium_0",
    "nodes": [
        {"nid": 0, "type": "fact", "label": "P0", "atom": "P0", "is_initial": True},
        {"nid": 1, "type": "fact", "label": "P1", "atom": "P1", "is_initial": True},
        {"nid": 2, "type": "rule", "label": "(P0 & P1) -> P2", "body_atoms": ["P0", "P1"], "head_atom": "P2"},
        {"nid": 3, "type": "fact", "label": "P2", "atom": "P2", "is_initial": False}
    ],
    "edges": [
        {"src": 0, "dst": 2, "etype": "body"},
        {"src": 1, "dst": 2, "etype": "body"},
        {"src": 2, "dst": 3, "etype": "head"}
    ],
    "proof_steps": [
        {"step_id": 0, "derived_node": 3, "used_rule": 2, "premises": [0, 1]}
    ],
    "metadata": {
        "difficulty": "medium", "n_nodes": 4, "n_edges": 3,
        "n_initial_facts": 2, "n_rules": 1, "proof_length": 1,
        "source": "test_generator"
    }
}

# Dummy spectral data
K_DIM = 8 # Keep small for testing
DUMMY_EIGVALS = np.linspace(0.01, 1.5, K_DIM).astype(np.float32)
DUMMY_EIGVECS = np.random.rand(DUMMY_INSTANCE["metadata"]["n_nodes"], K_DIM).astype(np.float32)

# Helper function for tensor shapes
def assertShapeEqual(test_case, tensor, expected_shape):
    test_case.assertEqual(tensor.shape, torch.Size(expected_shape))

class TestDataGenerator(unittest.TestCase):
    """Tests for data_generator.py"""

    def test_01_generate_horn_instance(self):
        print("\nTesting data_generator.generate_horn_instance...")
        instance = generate_horn_instance("test_gen_0", Difficulty.EASY, seed=42)
        self.assertIsInstance(instance, dict)
        self.assertIn("id", instance)
        self.assertIn("nodes", instance)
        self.assertIn("edges", instance)
        self.assertIn("proof_steps", instance)
        self.assertIn("metadata", instance)
        self.assertTrue(len(instance["nodes"]) > 0)
        self.assertTrue(len(instance["proof_steps"]) >= 0) # Can be 0 if trivial
        print("  ✓ Instance structure")

    def test_02_generate_dataset(self):
        print("\nTesting data_generator.generate_dataset...")
        if TEST_DATA_DIR.exists():
            shutil.rmtree(TEST_DATA_DIR) # Clean up previous runs
        TEST_DATA_DIR.mkdir(parents=True)

        n_config = {Difficulty.EASY: 2, Difficulty.MEDIUM: 1}
        stats = generate_dataset(str(TEST_DATA_DIR), n_config, seed=43)
        self.assertEqual(stats["total"], 3)
        self.assertEqual(stats["by_difficulty"]["easy"], 2)
        self.assertTrue((TEST_DATA_DIR / "easy" / "easy_0.json").exists())
        self.assertTrue((TEST_DATA_DIR / "medium" / "medium_0.json").exists())
        print("  ✓ Dataset generation and file output")
        # Clean up
        # shutil.rmtree(TEST_DATA_DIR)


class TestDatasetLoading(unittest.TestCase):
    """Tests for dataset.py"""

    @classmethod
    def setUpClass(cls):
        """Create dummy data files once for this test class."""
        print("\nSetting up TestDatasetLoading...")
        if TEST_DATA_DIR.exists(): # Clean slate
             shutil.rmtree(TEST_DATA_DIR)
        TEST_DATA_DIR.mkdir(parents=True)
        TEST_SPECTRAL_DIR.mkdir(parents=True)
        cls.test_files_for_split = []
        n_config = {Difficulty.EASY: 2, Difficulty.MEDIUM: 1}
        inst_counter = {'easy': 0, 'medium': 0}
        cls.test_files_for_split = []
        n_config = {Difficulty.EASY: 2, Difficulty.MEDIUM: 1}
        inst_counter = {'easy': 0, 'medium': 0}

        for diff, count in n_config.items():
            diff_dir = TEST_DATA_DIR / diff.value
            diff_dir.mkdir(exist_ok=True)
            for i in range(count):
                inst_id = f"{diff.value}_{i}"
                # Use a slightly modified DUMMY_INSTANCE structure for variety if needed
                temp_instance = copy.deepcopy(DUMMY_INSTANCE)
                temp_instance['id'] = inst_id
                temp_instance['metadata']['difficulty'] = diff.value
                file_path = diff_dir / f"{inst_id}.json"
                with open(file_path, 'w') as f:
                    json.dump(temp_instance, f)
                cls.test_files_for_split.append(str(file_path))
                print(f"  ✓ Created dummy instance: {file_path}")
                inst_counter[diff.value] += 1

        # Save DUMMY_INSTANCE_FILE path for other tests specifically needing it
        cls.single_dummy_file = str(TEST_DATA_DIR / "medium" / "medium_0.json")
        # --- END MODIFIED ---

        # Write dummy spectral file (use the specific medium_0 file)
        cls.dummy_spectral_path = TEST_SPECTRAL_DIR / f"{DUMMY_INSTANCE['id']}_spectral.npz"
        np.savez_compressed(
            cls.dummy_spectral_path,
            eigenvalues=DUMMY_EIGVALS,
            eigenvectors=DUMMY_EIGVECS,
            num_nodes=DUMMY_INSTANCE["metadata"]["n_nodes"],
            normalization='symmetric',
            k=K_DIM
        )
        print(f"  ✓ Created dummy spectral cache: {cls.dummy_spectral_path}")


    @classmethod
    def tearDownClass(cls):
        """Clean up dummy data files."""
        print("\nTearing down TestDatasetLoading...")
        if TEST_DATA_DIR.exists():
            shutil.rmtree(TEST_DATA_DIR)
        print("  ✓ Cleaned up test data directory")

    def setUp(self):
        """Load dataset instance for each test."""
        self.dataset_no_spectral = StepPredictionDataset([self.single_dummy_file])
        self.dataset_with_spectral = StepPredictionDataset(
            [self.single_dummy_file], spectral_dir=str(TEST_SPECTRAL_DIR)
        )
        self.num_nodes = DUMMY_INSTANCE["metadata"]["n_nodes"]
        self.expected_base_features = 22

    def test_01_dataset_init(self):
        print("\nTesting dataset.StepPredictionDataset initialization...")
        self.assertEqual(len(self.dataset_no_spectral), len(DUMMY_INSTANCE["proof_steps"]))
        self.assertEqual(len(self.dataset_with_spectral), len(DUMMY_INSTANCE["proof_steps"]))
        self.assertEqual(self.dataset_with_spectral.k_default, K_DIM)
        print("  ✓ Initialization and length")

    def test_02_compute_node_features(self):
        print("\nTesting dataset._compute_node_features...")
        features = self.dataset_no_spectral._compute_node_features(DUMMY_INSTANCE, 0)
        self.assertIsInstance(features, torch.Tensor)
        assertShapeEqual(self, features, (self.num_nodes, self.expected_base_features))
        # Simple checks on feature values (e.g., type encoding)
        self.assertEqual(features[0, 0], 1.0) # Node 0 is fact
        self.assertEqual(features[2, 1], 1.0) # Node 2 is rule
        self.assertEqual(features[0, 2], 1.0) # Node 0 is initial
        self.assertEqual(features[3, 2], 0.0) # Node 3 is not initial
        print("  ✓ Feature computation and shape")

    def test_03_load_spectral(self):
        print("\nTesting dataset._load_spectral...")
        # With spectral dir
        eigvecs, eigvals = self.dataset_with_spectral._load_spectral(DUMMY_INSTANCE["id"], True)
        self.assertIsInstance(eigvecs, torch.Tensor)
        self.assertIsInstance(eigvals, torch.Tensor)
        assertShapeEqual(self, eigvecs, (self.num_nodes, K_DIM))
        assertShapeEqual(self, eigvals, (K_DIM,))

        # Without spectral dir
        eigvecs_none, eigvals_none = self.dataset_no_spectral._load_spectral(DUMMY_INSTANCE["id"], False)
        self.assertIsNone(eigvecs_none)
        self.assertIsNone(eigvals_none)
        print("  ✓ Spectral loading (present and absent)")

    def test_04_getitem_no_spectral(self):
        print("\nTesting dataset.__getitem__ (no spectral)...")
        data = self.dataset_no_spectral[0]
        self.assertIsInstance(data, Data)
        self.assertTrue(hasattr(data, 'x'))
        self.assertTrue(hasattr(data, 'edge_index'))
        self.assertTrue(hasattr(data, 'edge_attr'))
        self.assertTrue(hasattr(data, 'y'))
        self.assertTrue(hasattr(data, 'derived_mask'))
        self.assertTrue(hasattr(data, 'step_numbers'))
        self.assertTrue(hasattr(data, 'value_target'))
        self.assertTrue(hasattr(data, 'difficulty'))
        self.assertTrue(hasattr(data, 'tactic_target'))
        self.assertTrue(hasattr(data, 'eigvecs')) # Should have zeros
        self.assertTrue(hasattr(data, 'eigvals')) # Should have zeros

        assertShapeEqual(self, data.x, (self.num_nodes, self.expected_base_features))
        assertShapeEqual(self, data.edge_index, (2, len(DUMMY_INSTANCE["edges"])))
        assertShapeEqual(self, data.y, (1,))
        assertShapeEqual(self, data.derived_mask, (self.num_nodes,))
        assertShapeEqual(self, data.step_numbers, (self.num_nodes,))
        assertShapeEqual(self, data.value_target, (1,))
        assertShapeEqual(self, data.difficulty, (1,))
        assertShapeEqual(self, data.tactic_target, (1,))
        # Check padding for missing spectral
        assertShapeEqual(self, data.eigvecs, (self.num_nodes, self.dataset_no_spectral.k_default))
        assertShapeEqual(self, data.eigvals, (self.dataset_no_spectral.k_default,))
        self.assertEqual(data.y.item(), id2idx(DUMMY_INSTANCE, DUMMY_INSTANCE["proof_steps"][0]["used_rule"]))
        print("  ✓ Data object structure and shapes (no spectral)")

    def test_05_getitem_with_spectral(self):
        print("\nTesting dataset.__getitem__ (with spectral)...")
        data = self.dataset_with_spectral[0]
        self.assertIsInstance(data, Data)
        self.assertTrue(hasattr(data, 'eigvecs'))
        self.assertTrue(hasattr(data, 'eigvals'))
        assertShapeEqual(self, data.eigvecs, (self.num_nodes, K_DIM))
        assertShapeEqual(self, data.eigvals, (K_DIM,))
        # Check that loaded values are not all zero (unlike the padding case)
        self.assertTrue(torch.any(data.eigvecs != 0))
        self.assertTrue(torch.any(data.eigvals != 0))
        print("  ✓ Data object structure and shapes (with spectral)")

    def test_06_create_split(self):
        print("\nTesting dataset.create_split...")
        # Use the TEST_DATA_DIR where multiple files were created in setUpClass
        temp_data_dir = str(TEST_DATA_DIR)
        train_f, val_f, test_f = create_split(temp_data_dir, train_ratio=0.4, val_ratio=0.3, test_ratio=0.3, seed=44)
        # We created 3 files (2 easy, 1 medium)
        self.assertEqual(len(train_f), 1) # int(3 * 0.4) = 1
        self.assertEqual(len(val_f), 1)   # int(3 * 0.3) = 0 -> adjusted split logic usually ensures at least 1 if possible, but let's recheck calculation: n_train=1, n_val=int(0.9)=0. Indices: train=[0], val=[], test=[1, 2]. Ah, the split logic takes floors. Let's adjust expected:
        # n=3, train_ratio=0.4 -> n_train = int(1.2)=1
        # val_ratio=0.3 -> n_val = int(0.9)=0
        # test takes the rest -> n_test = 3 - 1 - 0 = 2
        self.assertEqual(len(train_f), 1)
        self.assertEqual(len(val_f), 0) # Floor calculation means 0
        self.assertEqual(len(test_f), 2) # Remaining 2
        print("  ✓ Instance-level splitting (corrected expectations for floor)")

    def test_07_rule_to_tactic_mapper(self):
         print("\nTesting dataset.RuleToTacticMapper...")
         mapper = RuleToTacticMapper()
         tactic_idx = mapper.map_rule_to_tactic("(P0 & P1) -> P2") # Matches '->'
         self.assertEqual(mapper.tactic_names[tactic_idx], 'forward_chain')
         tactic_idx_unknown = mapper.map_rule_to_tactic("some_other_rule")
         self.assertEqual(mapper.tactic_names[tactic_idx_unknown], 'unknown')
         print("  ✓ Tactic mapping")


class TestLosses(unittest.TestCase):
    """Tests for losses.py"""

    def test_01_proof_search_ranking_loss(self):
        print("\nTesting losses.ProofSearchRankingLoss...")
        loss_fn = ProofSearchRankingLoss(margin=1.0)
        # Scores: [correct, hard_neg, hard_neg, easy_neg, easy_neg]
        # Target index is 0
        scores = torch.tensor([2.5, 2.0, 1.8, 0.5, 0.1], requires_grad=True)        
        embeddings = torch.randn(5, 10) # Not used in this version
        target_idx = 0
        loss = loss_fn(scores, embeddings, target_idx)

        self.assertIsInstance(loss, torch.Tensor)
        self.assertTrue(loss.requires_grad)
        # Expected: margin=1. Violations:
        # Hard1: relu(1 - (2.5 - 2.0)) = relu(0.5) = 0.5
        # Hard2: relu(1 - (2.5 - 1.8)) = relu(0.3) = 0.3
        # Easy1: relu(1 - (2.5 - 0.5)) = relu(-1.0) = 0.0
        # Easy2: relu(1 - (2.5 - 0.1)) = relu(-1.4) = 0.0
        # Hard mean = (0.5+0.3)/2 = 0.4. Easy mean = (0+0)/2 = 0.0
        # Loss = 0.7 * 0.4 + 0.3 * 0.0 = 0.28
        self.assertAlmostEqual(loss.item(), 0.28, places=4)
        print("  ✓ Loss calculation with hard/easy split")

        # Test edge case: no negatives
        loss_no_neg = loss_fn(scores[0:1], embeddings[0:1], 0)
        self.assertEqual(loss_no_neg.item(), 0.0)
        print("  ✓ Edge case: no negatives")

    def test_02_compute_hit_at_k(self):
        print("\nTesting losses.compute_hit_at_k...")
        scores = torch.tensor([3.0, 1.0, 4.0, 2.0]) # Ranks: 2, 0, 3, 1
        self.assertEqual(compute_hit_at_k(scores, 2, 1), 1.0) # Hit@1 (target is highest)
        self.assertEqual(compute_hit_at_k(scores, 0, 1), 0.0) # Miss@1
        self.assertEqual(compute_hit_at_k(scores, 0, 2), 1.0) # Hit@2 (target is 2nd highest)
        self.assertEqual(compute_hit_at_k(scores, 1, 3), 0.0) # Miss@3
        self.assertEqual(compute_hit_at_k(scores, 1, 4), 1.0) # Hit@4
        self.assertEqual(compute_hit_at_k(scores, 5, 1), 0.0) # Invalid target index
        print("  ✓ Hit@K calculation")

    def test_03_compute_mrr(self):
        print("\nTesting losses.compute_mrr...")
        scores = torch.tensor([3.0, 1.0, 4.0, 2.0]) # Ranks: 2(1st), 0(2nd), 3(3rd), 1(4th)
        self.assertAlmostEqual(compute_mrr(scores, 2), 1.0 / 1.0) # Rank 1
        self.assertAlmostEqual(compute_mrr(scores, 0), 1.0 / 2.0) # Rank 2
        self.assertAlmostEqual(compute_mrr(scores, 3), 1.0 / 3.0) # Rank 3
        self.assertAlmostEqual(compute_mrr(scores, 1), 1.0 / 4.0) # Rank 4
        self.assertEqual(compute_mrr(scores, 5), 0.0) # Invalid target index
        print("  ✓ MRR calculation")


class TestModelComponents(unittest.TestCase):
    """Tests for model.py components"""
    def setUp(self):
        self.hidden_dim = 64
        self.k_dim = K_DIM
        self.num_nodes = 10
        self.num_edges = 20
        self.in_dim = 22 # From dataset
        self.num_tactics = 6 # From mapper
        self.device = 'cpu'

        # Dummy inputs matching expected shapes
        self.x = torch.randn(self.num_nodes, self.in_dim)
        self.edge_index = torch.randint(0, self.num_nodes, (2, self.num_edges))
        self.edge_attr = torch.randint(0, 3, (self.num_edges,))
        self.derived_mask = torch.randint(0, 2, (self.num_nodes,)).bool()
        self.step_numbers = torch.randint(0, 10, (self.num_nodes,))
        self.eigvecs = torch.randn(self.num_nodes, self.k_dim)
        self.eigvals = torch.randn(self.k_dim)
        self.batch = torch.zeros(self.num_nodes, dtype=torch.long) # Single graph

    def test_01_improved_spectral_filter(self):
        print("\nTesting model.ImprovedSpectralFilter...")
        filt = ImprovedSpectralFilter(self.in_dim, self.hidden_dim, self.k_dim)
        out = filt(self.x, self.eigvecs, self.eigvals)
        assertShapeEqual(self, out, (self.num_nodes, self.hidden_dim))
        print("  ✓ Forward pass and output shape")

    def test_02_type_aware_gatv2conv(self):
        print("\nTesting model.TypeAwareGATv2Conv...")
        # Needs node_types (first 2 cols of x)
        node_types = self.x[:, :2]
        conv = TypeAwareGATv2Conv(self.in_dim, self.hidden_dim, heads=4, edge_dim=None)
        # Note: This conv expects input dim = hidden_dim in NodeRankingGNN,
        # here we test its standalone forward pass logic
        out = conv(self.x, self.edge_index, node_types)
        assertShapeEqual(self, out, (self.num_nodes, self.hidden_dim))
        print("  ✓ Forward pass and output shape")

    def test_03_node_ranking_gnn(self):
        print("\nTesting model.NodeRankingGNN...")
        gnn = NodeRankingGNN(self.in_dim, self.hidden_dim, num_layers=2)
        scores, h = gnn(self.x, self.edge_index, self.edge_attr, self.batch)
        assertShapeEqual(self, scores, (self.num_nodes,))
        assertShapeEqual(self, h, (self.num_nodes, self.hidden_dim))
        print("  ✓ Forward pass and output shapes (scores, embeddings)")

    def test_04_fixed_temporal_spectral_gnn(self):
        print("\nTesting model.FixedTemporalSpectralGNN...")
        model = FixedTemporalSpectralGNN(self.in_dim, self.hidden_dim, k=self.k_dim)
        scores, embeddings, value = model(
            self.x, self.edge_index, self.derived_mask, self.step_numbers,
            self.eigvecs, self.eigvals, self.edge_attr, self.batch
        )
        assertShapeEqual(self, scores, (self.num_nodes,))
        assertShapeEqual(self, embeddings, (self.num_nodes, self.hidden_dim * 3))
        assertShapeEqual(self, value, (1,)) # Value is per-graph
        print("  ✓ Forward pass and output shapes (scores, embeddings, value)")

    def test_05_tactic_classifier(self):
        print("\nTesting model.TacticClassifier...")
        classifier = TacticClassifier(self.hidden_dim, num_tactics=self.num_tactics)
        graph_embedding = torch.randn(1, self.hidden_dim * 3) # Dummy graph embedding
        logits = classifier(graph_embedding)
        assertShapeEqual(self, logits, (1, self.num_tactics))
        print("  ✓ Forward pass and output shape")

    def test_06_tactic_guided_gnn(self):
        print("\nTesting model.TacticGuidedGNN...")
        model = TacticGuidedGNN(self.in_dim, self.hidden_dim, k=self.k_dim, num_tactics=self.num_tactics)
        scores, embeddings, value, tactic_logits = model(
            self.x, self.edge_index, self.derived_mask, self.step_numbers,
            self.eigvecs, self.eigvals, self.edge_attr, self.batch
        )
        assertShapeEqual(self, scores, (self.num_nodes,))
        assertShapeEqual(self, embeddings, (self.num_nodes, self.hidden_dim * 3))
        assertShapeEqual(self, value, (1,))
        assertShapeEqual(self, tactic_logits, (1, self.num_tactics)) # Tactic logits per graph
        print("  ✓ Forward pass and output shapes (scores, embeddings, value, tactic_logits)")

    def test_07_premise_selector(self):
        print("\nTesting model.PremiseSelector...")
        selector = PremiseSelector(self.hidden_dim)
        fact_embeddings = torch.randn(self.num_nodes, self.hidden_dim * 3)
        graph_embedding = torch.randn(1, self.hidden_dim * 3)
        relevance = selector.score_fact_relevance(fact_embeddings, graph_embedding)
        assertShapeEqual(self, relevance, (self.num_nodes,))
        self.assertTrue(torch.all(relevance >= 0) and torch.all(relevance <= 1))
        print("  ✓ Forward pass and output shape")

    def test_08_get_model(self):
        print("\nTesting model.get_model factory...")
        model = get_model(self.in_dim, self.hidden_dim, num_layers=2, k=self.k_dim, num_tactics=self.num_tactics)
        self.assertIsInstance(model, TacticGuidedGNN)
        print("  ✓ Model creation")

class TestSpectralFeatures(unittest.TestCase):
    """Tests for spectral_features.py"""
    def setUp(self):
        # Create a simple graph (triangle with one isolated node)
        self.num_nodes = 4
        # Edges: 0-1, 1-2, 2-0
        self.edge_index = torch.tensor([[0, 1, 1, 2, 2, 0], [1, 0, 2, 1, 0, 2]], dtype=torch.long)
        self.k = 3
        self.extractor = SpectralFeatureExtractor(k=self.k, add_self_loops=False) # No loops for easier checks

    def test_01_compute_normalized_laplacian(self):
        print("\nTesting spectral_features.compute_normalized_laplacian...")
        laplacian = self.extractor.compute_normalized_laplacian(self.edge_index, self.num_nodes)
        self.assertEqual(laplacian.shape, (self.num_nodes, self.num_nodes))
        # Check properties (e.g., zero row/col sum for symmetric, approx for others)
        # Check diagonal entries, off-diagonal
        print("  ✓ Laplacian calculation and shape")

    def test_02_eigenvalue_correction(self):
        print("\nTesting spectral_features._eigenvalue_correction...")
        # Test case: preserve zero, perturb repeated non-zero
        eigvals = np.array([0.0, 1e-6, 0.5, 0.5, 0.5, 1.0])
        corrected = self.extractor._eigenvalue_correction(eigvals, epsilon=1e-3)
        self.assertAlmostEqual(corrected[0], 0.0)
        self.assertAlmostEqual(corrected[1], 1e-6) # Structural zero preserved
        self.assertNotAlmostEqual(corrected[3], corrected[2]) # Repeated perturbed
        self.assertNotAlmostEqual(corrected[4], corrected[3]) # Repeated perturbed
        self.assertAlmostEqual(corrected[5], 1.0)
        print("  ✓ Eigenvalue correction logic (zero preservation, perturbation)")

    def test_03_compute_spectral_decomposition(self):
        print("\nTesting spectral_features.compute_spectral_decomposition...")
        laplacian = self.extractor.compute_normalized_laplacian(self.edge_index, self.num_nodes)
        eigvals, eigvecs = self.extractor.compute_spectral_decomposition(laplacian, k=self.k)
        self.assertEqual(len(eigvals), self.k)
        self.assertEqual(eigvecs.shape, (self.num_nodes, self.k))
        # Expect first eigenvalue near zero for connected graph part
        self.assertTrue(np.all(eigvals >= -1e-9)) # Should be non-negative
        print("  ✓ Spectral decomposition shape and basic properties")

    def test_04_extract_features(self):
        print("\nTesting spectral_features.extract_features...")
        features = self.extractor.extract_features(self.edge_index, self.num_nodes, validate=True)
        self.assertIn('eigenvalues', features)
        self.assertIn('eigenvectors', features)
        self.assertIn('validation', features)
        self.assertEqual(len(features['eigenvalues']), self.k)
        self.assertEqual(features['eigenvectors'].shape, (self.num_nodes, self.k))
        self.assertTrue(features['validation']['passed'])
        print("  ✓ Feature extraction output structure and validation")

    def test_05_load_spectral_features(self):
        print("\nTesting spectral_features.load_spectral_features...")
        # --- MODIFIED: Use the path stored during setUpClass ---
        dummy_file_path = TestDatasetLoading.dummy_spectral_path # Use class attribute
        if not dummy_file_path.exists():
             self.skipTest(f"Dummy spectral file not found ({dummy_file_path}), skipping load test.")

        
        loaded_features = load_spectral_features(str(DUMMY_SPECTRAL_FILE))
        self.assertIn('eigenvalues', loaded_features)
        self.assertIn('eigenvectors', loaded_features)
        self.assertEqual(len(loaded_features['eigenvalues']), K_DIM)
        self.assertEqual(loaded_features['num_nodes'], DUMMY_INSTANCE["metadata"]["n_nodes"])
        print("  ✓ Loading from .npz file")

class TestTemporalEncoder(unittest.TestCase):
    """Tests for temporal_encoder.py"""
    def setUp(self):
        self.hidden_dim = 64
        self.num_nodes = 10
        self.max_steps = 20
        self.num_heads = 4
        self.frontier_window = 5

        # Dummy inputs
        self.derived_mask = torch.randint(0, 2, (self.num_nodes,)).bool()
        self.step_numbers = torch.randint(0, self.max_steps, (self.num_nodes,))
        self.node_features = torch.randn(self.num_nodes, self.hidden_dim)

    def test_01_fixed_time_encoding(self):
        print("\nTesting temporal_encoder.FixedTimeEncoding...")
        encoder = FixedTimeEncoding(self.hidden_dim // 2, self.max_steps) # Dim must be even
        encoding = encoder(self.step_numbers)
        assertShapeEqual(self, encoding, (self.num_nodes, self.hidden_dim // 2))
        print("  ✓ Forward pass and output shape")

    def test_02_proof_frontier_attention(self):
        print("\nTesting temporal_encoder.ProofFrontierAttention...")
        attn = ProofFrontierAttention(self.hidden_dim, self.num_heads, self.frontier_window)
        max_step = self.step_numbers.max().item()
        attended, weights = attn(self.node_features, self.step_numbers, max_step)
        assertShapeEqual(self, attended, (self.num_nodes, self.hidden_dim))
        assertShapeEqual(self, weights, (self.num_nodes, self.num_nodes))
        print("  ✓ Forward pass and output shapes")

    def test_03_temporal_state_encoder(self):
        print("\nTesting temporal_encoder.TemporalStateEncoder...")
        encoder = TemporalStateEncoder(self.hidden_dim, self.num_heads, self.frontier_window, self.max_steps)
        encoding, weights = encoder(
            self.derived_mask.long(), self.step_numbers, self.node_features, return_attention=True
        )
        assertShapeEqual(self, encoding, (self.num_nodes, self.hidden_dim))
        self.assertIsNotNone(weights)
        assertShapeEqual(self, weights, (self.num_nodes, self.num_nodes))
        print("  ✓ Forward pass and output shapes")

    def test_04_multi_scale_temporal_encoder(self):
        print("\nTesting temporal_encoder.MultiScaleTemporalEncoder...")
        encoder = MultiScaleTemporalEncoder(self.hidden_dim, num_scales=3, max_steps=self.max_steps)
        encoding = encoder(self.derived_mask.long(), self.step_numbers, self.node_features)
        assertShapeEqual(self, encoding, (self.num_nodes, self.hidden_dim))
        print("  ✓ Forward pass and output shape")

    def test_05_utility_functions(self):
        print("\nTesting temporal_encoder utility functions...")
        # Dummy proof state for utilities
        proof_state = {
            'num_nodes': 5,
            'derivations': [(3, 1), (4, 2)] # Node 3 derived at step 1, Node 4 at step 2
        }
        mask = compute_derived_mask(proof_state, current_step=2)
        steps = compute_step_numbers(proof_state, current_step=2)
        assertShapeEqual(self, mask, (5,))
        assertShapeEqual(self, steps, (5,))
        self.assertTrue(torch.equal(mask.long(), torch.tensor([0, 0, 0, 1, 1])))
        self.assertTrue(torch.equal(steps, torch.tensor([0, 0, 0, 1, 2])))
        print("  ✓ compute_derived_mask and compute_step_numbers")


class TestSearchAgent(unittest.TestCase):
    """Tests for search_agent.py"""

    @classmethod
    def setUpClass(cls):
        # Need a dataset instance to initialize simulator
        if not DUMMY_INSTANCE_FILE.exists():
             # Re-create if deleted by other tests
             if TEST_DATA_DIR.exists(): shutil.rmtree(TEST_DATA_DIR)
             TEST_DATA_DIR.mkdir(parents=True)
             with open(DUMMY_INSTANCE_FILE, 'w') as f: json.dump(DUMMY_INSTANCE, f)

        cls.dataset_instance = StepPredictionDataset([str(DUMMY_INSTANCE_FILE)])

    def setUp(self):
        # Need a dummy model
        self.in_dim = 22
        self.hidden_dim = 32 # Small hidden dim for tests
        self.k_dim = K_DIM
        self.num_tactics = self.dataset_instance.num_tactics
        self.dummy_model = TacticGuidedGNN(self.in_dim, self.hidden_dim, k=self.k_dim, num_tactics=self.num_tactics)

    def test_01_proof_simulator_init(self):
        print("\nTesting search_agent.ProofSimulator initialization...")
        simulator = ProofSimulator(self.dataset_instance)
        self.assertIsNotNone(simulator.dataset)
        print("  ✓ Initialization")

    def test_02_simulator_get_initial_state(self):
        print("\nTesting search_agent.ProofSimulator.get_initial_state...")
        simulator = ProofSimulator(self.dataset_instance)
        initial_data = simulator.get_initial_state(DUMMY_INSTANCE)
        self.assertIsInstance(initial_data, Data)
        self.assertEqual(initial_data.current_step_simulated, 0)
        self.assertTrue(hasattr(initial_data, 'instance_data'))
        print("  ✓ Initial state creation")

    def test_03_simulator_apply_rule_placeholder(self):
         print("\nTesting search_agent.ProofSimulator.apply_rule (placeholder)...")
         simulator = ProofSimulator(self.dataset_instance)
         initial_data = simulator.get_initial_state(DUMMY_INSTANCE)
         self.assertIsNotNone(initial_data)
         # Rule node index for the dummy instance's rule is 2
         rule_node_idx = 2
         next_data = simulator.apply_rule(initial_data, rule_node_idx)
         # Basic check that it returns *something* and increments step
         self.assertIsInstance(next_data, Data)
         self.assertEqual(next_data.current_step_simulated, 1)
         # Check derived mask (node 3 should be derived)
         self.assertEqual(next_data.derived_mask[3], 1)
         print("  ✓ Placeholder apply_rule returns Data and updates step/mask")

    def test_04_simulator_get_applicable_rules(self):
         print("\nTesting search_agent.ProofSimulator.get_applicable_rules_indices...")
         simulator = ProofSimulator(self.dataset_instance)
         initial_data = simulator.get_initial_state(DUMMY_INSTANCE)
         self.assertIsNotNone(initial_data)
         applicable = simulator.get_applicable_rules_indices(initial_data)
         # In the initial state, rule node 2 should be applicable
         self.assertEqual(applicable, [2])

         # Simulate applying rule 2
         next_data = simulator.apply_rule(initial_data, 2)
         self.assertIsNotNone(next_data)
         applicable_after = simulator.get_applicable_rules_indices(next_data)
         # No more rules should be applicable
         self.assertEqual(applicable_after, [])
         print("  ✓ Applicable rule identification")

    def test_05_beam_search_ranker_init(self):
        print("\nTesting search_agent.BeamSearchRanker initialization...")
        ranker = BeamSearchRanker(self.dummy_model, self.dataset_instance, beam_width=3)
        self.assertEqual(ranker.beam_width, 3)
        self.assertIsNotNone(ranker.simulator)
        print("  ✓ Initialization")

    def test_06_beam_search_placeholder(self):
         print("\nTesting search_agent.BeamSearchRanker.search (placeholder)...")
         # This test ensures the search runs without crashing, given the simulator placeholder.
         # It won't find a real proof yet.
         ranker = BeamSearchRanker(self.dummy_model, self.dataset_instance, beam_width=2)
         # Run search - expects instance dictionary
         best_path, best_score = ranker.search(DUMMY_INSTANCE, max_depth=2)

         # Check if it returned a path (list of indices) or None
         self.assertTrue(isinstance(best_path, list) or best_path is None)
         self.assertIsInstance(best_score, float)
         # In the dummy case, the only path is applying rule 2
         if best_path is not None:
              self.assertEqual(best_path, [2])
         print("  ✓ Search runs with placeholder simulator")

class TestTrainingProcess(unittest.TestCase):
    """Tests for train.py components"""

    @classmethod
    def setUpClass(cls):
        # Create dummy data if needed
        if not DUMMY_INSTANCE_FILE.exists():
             if TEST_DATA_DIR.exists(): shutil.rmtree(TEST_DATA_DIR)
             TEST_DATA_DIR.mkdir(parents=True)
             with open(DUMMY_INSTANCE_FILE, 'w') as f: json.dump(DUMMY_INSTANCE, f)

        cls.dataset_instance = StepPredictionDataset([str(DUMMY_INSTANCE_FILE)])
        # Use very small model for faster testing
        cls.in_dim = 22
        cls.hidden_dim = 16
        cls.k_dim = K_DIM
        cls.num_tactics = cls.dataset_instance.num_tactics
        cls.model = TacticGuidedGNN(cls.in_dim, cls.hidden_dim, num_layers=1, k=cls.k_dim, num_tactics=cls.num_tactics)
        cls.optimizer = torch.optim.Adam(cls.model.parameters(), lr=1e-3)
        cls.criterion = ProofSearchRankingLoss()
        cls.device = 'cpu'

    def test_01_curriculum_scheduler(self):
        print("\nTesting train.CurriculumScheduler...")
        scheduler = CurriculumScheduler(self.dataset_instance, base_difficulty=0.1, max_difficulty=0.9)
        # Test index fetching at different epochs
        indices_start = scheduler.get_batch_indices(epoch=0, total_epochs=10)
        indices_mid = scheduler.get_batch_indices(epoch=5, total_epochs=10)
        indices_end = scheduler.get_batch_indices(epoch=10, total_epochs=10)

        self.assertTrue(len(indices_start) > 0)
        # Expect more samples as curriculum progresses (if data allows)
        self.assertTrue(len(indices_mid) >= len(indices_start))
        self.assertTrue(len(indices_end) >= len(indices_mid))
        print("  ✓ Index retrieval and progression")

    def test_02_get_lr(self):
         print("\nTesting train.get_lr...")
         lr = get_lr(self.optimizer)
         self.assertAlmostEqual(lr, 1e-3)
         # Change LR and check again
         for param_group in self.optimizer.param_groups:
              param_group['lr'] = 5e-4
         lr_new = get_lr(self.optimizer)
         self.assertAlmostEqual(lr_new, 5e-4)
         # Reset LR
         for param_group in self.optimizer.param_groups:
              param_group['lr'] = 1e-3
         print("  ✓ Learning rate retrieval")

    @unittest.skip("Skipping train_epoch/evaluate integration test - complex setup")
    def test_03_train_epoch_evaluate_integration(self):
        print("\nTesting train.train_epoch and train.evaluate (integration)...")
        # This requires a DataLoader and running a mini-loop. It's more of an
        # integration test than a unit test.
        # We'll just check if the functions can be called without error.
        sampler = SubsetRandomSampler([0]) # Use only the first sample
        loader = DataLoader(self.dataset_instance, batch_size=1, sampler=sampler)

        try:
             train_loss, train_val_loss, train_tac_loss, train_acc = train_epoch(
                  self.model, loader, self.optimizer, self.criterion, self.device, epoch=1,
                  grad_accum_steps=1, value_loss_weight=0.1, tactic_loss_weight=0.1
             )
             val_metrics = evaluate(self.model, loader, self.criterion, self.device, 'val')

             self.assertIsInstance(train_loss, float)
             self.assertIsInstance(train_acc, float)
             self.assertIsInstance(val_metrics, dict)
             print("  ✓ Functions callable without errors")
        except Exception as e:
             self.fail(f"train_epoch/evaluate integration failed: {e}")

# --- Helper Functions ---
def id2idx(instance, nid):
    """Maps node ID to its index in the nodes list."""
    for i, node in enumerate(instance["nodes"]):
        if node["nid"] == nid:
            return i
    return -1

# --- Test Runner ---
if __name__ == '__main__':
    print("=" * 70)
    print(" RUNNING HORN CLAUSE GNN TEST SUITE")
    print("=" * 70)
    unittest.main(verbosity=1) # Use verbosity=2 for more detailed output